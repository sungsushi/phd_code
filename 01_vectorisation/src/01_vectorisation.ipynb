{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import itertools\n",
    "# import graphviz\n",
    "from matplotlib.pyplot import cm\n",
    "from matplotlib.gridspec import GridSpec, GridSpecFromSubplotSpec\n",
    "import re\n",
    "# import plotly.express as px\n",
    "\n",
    "# import networkx as nx\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster, to_tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.dendorgram_utils import dendrogram_clustering\n",
    "from module.data_prep import prep_grn_data, prep_go_meta, prep_foodweb_data, prep_recipe_data\n",
    "# from module.bipartite_vectorisation import get_bpt_dict, all_ud_bpt_vectors\n",
    "# from module.mat_bpt_vec import *\n",
    "\n",
    "from fvec.fvec import bipartite_cooarray, adjacency_cooarray, csr_row_norm, csr_col_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_things = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssm47/Desktop/thesis_code/01_vectorisation/src/module/data_prep.py:5: DtypeWarning: Columns (12,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('../../data/grn/AtRegNet.csv', index_col=0)\n"
     ]
    }
   ],
   "source": [
    "# Arabidopsis gene regulatory network and associated metadata\n",
    "g_reg_edges, GO_bipartite_df = prep_grn_data()\n",
    "go_meta_df = prep_go_meta()\n",
    "\n",
    "GO_meta = pd.DataFrame({'node':GO_bipartite_df['GO'], 'type':GO_bipartite_df['GO']}) # dummy dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AT5G10140</td>\n",
       "      <td>AT1G65480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AT5G11260</td>\n",
       "      <td>AT1G27480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AT5G11260</td>\n",
       "      <td>AT5G53370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AT5G11260</td>\n",
       "      <td>AT1G03630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AT5G11260</td>\n",
       "      <td>AT1G13600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638795</th>\n",
       "      <td>AT5G13080</td>\n",
       "      <td>AT5G67540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638796</th>\n",
       "      <td>AT5G13080</td>\n",
       "      <td>AT5G67570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638797</th>\n",
       "      <td>AT5G13080</td>\n",
       "      <td>AT5G67620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638798</th>\n",
       "      <td>AT5G13080</td>\n",
       "      <td>AT5G67630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638799</th>\n",
       "      <td>AT5G13080</td>\n",
       "      <td>AT5G67640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1636423 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               pre       post\n",
       "0        AT5G10140  AT1G65480\n",
       "1        AT5G11260  AT1G27480\n",
       "2        AT5G11260  AT5G53370\n",
       "3        AT5G11260  AT1G03630\n",
       "4        AT5G11260  AT1G13600\n",
       "...            ...        ...\n",
       "1638795  AT5G13080  AT5G67540\n",
       "1638796  AT5G13080  AT5G67570\n",
       "1638797  AT5G13080  AT5G67620\n",
       "1638798  AT5G13080  AT5G67630\n",
       "1638799  AT5G13080  AT5G67640\n",
       "\n",
       "[1636423 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_reg_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorisation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRN vectorisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_things:\n",
    "    grn_datadir = '../../data/grn'\n",
    "    grn_processed_dir = grn_datadir + '/processed'\n",
    "    if not os.path.isdir(grn_processed_dir):\n",
    "        os.makedirs(grn_processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs = g_reg_edges.pre.unique() # all transcription factors\n",
    "regulated_genes = g_reg_edges.post.unique() # all genes that are regulated\n",
    "g_reg_ids = list(set(regulated_genes) | set(tfs)) # all nodes in the regulatory network\n",
    "gene_ids = list(set(GO_bipartite_df.gene)) # genes that have GO labelling\n",
    "\n",
    "all_gene_ids = list(set(regulated_genes) | set(tfs) | set(GO_bipartite_df.gene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31031"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gene_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # aggregate all GO terms for each gene and save it\n",
    "# pre_post= ['gene', 'GO', 'gene', 'GO']\n",
    "# fpath_prefix = grn_processed_dir + '/go_list_dict'\n",
    "# go_dict = get_bpt_dict(ids=gene_ids, bpt_df=GO_bipartite_df,pre_post=pre_post, fpath_prefix=fpath_prefix)\n",
    "\n",
    "# # ~4.5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = GO_bipartite_df.GO.value_counts()\n",
    "single_GOs = v[v <=1].index.tolist() # GO terms that only appear once\n",
    "multi_GOs = v[v >1].index.tolist() # GO terms that appear more than once\n",
    "\n",
    "\n",
    "# only consider GO terms that appear more than once:\n",
    "# prefix = grn_processed_dir + '/grn_GO_vectors_mutligos' # all the gene ids in the regulatory network, in/out GO vectors, ignore single GO terms.\n",
    "# grn_go_vectors = all_ud_bpt_vectors(ids=g_reg_ids, edges=g_reg_edges, bpt_dict=go_dict, keep=multi_GOs, prefix=prefix)\n",
    "\n",
    "# # ~28 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO_bipartite_multigos_df = GO_bipartite_df[GO_bipartite_df['GO'].isin(multi_GOs)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create COO_arrays from edgelists\n",
    "bpt_gG_coo, gene_row, GO_col = bipartite_cooarray(df=GO_bipartite_multigos_df, row_col=['gene', 'GO'], weight=False, row_order=all_gene_ids)\n",
    "a_gg_coo, _ = adjacency_cooarray(df=g_reg_edges, row_col=['pre', 'post'], id_order=all_gene_ids, weight=False, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct vector elements by the bipartite anntoations:\n",
    "a_gG_out = (a_gg_coo @ bpt_gG_coo).tocsr() # out matrix\n",
    "a_gG_in = (a_gg_coo.T @ bpt_gG_coo).tocsr() # in matrix\n",
    "\n",
    "\n",
    "# out matrix normalisation:\n",
    "a_gG_out_normalised = csr_row_norm(a_gG_out)\n",
    "# in matrix normalisation:\n",
    "a_gG_in_normalised = csr_row_norm(a_gG_in)\n",
    "\n",
    "# put together into one dataframe:\n",
    "all_col_names = np.concatenate([\n",
    "    np.char.add(GO_col.astype(str), '_out'),\n",
    "    np.char.add(GO_col.astype(str), '_in')\n",
    "])\n",
    "all_norm_vec_df = pd.DataFrame(sp.sparse.hstack([a_gG_out_normalised, a_gG_in_normalised]).toarray(), columns=all_col_names, index=all_gene_ids)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct vector elements by the bipartite anntoations:\n",
    "# a_gG_out = a_gg_coo.dot(bpt_gG_coo).tocsr() # out matrix\n",
    "# a_gG_in = a_gg_coo.T.dot(bpt_gG_coo).tocsr() # in matrix\n",
    "\n",
    "# fast implementation of a dot product:\n",
    "a_gG_out = (a_gg_coo @ bpt_gG_coo).tocsr() # out matrix\n",
    "a_gG_in = (a_gg_coo.T @ bpt_gG_coo).tocsr() # in matrix\n",
    "\n",
    "\n",
    "# out matrix normalisation:\n",
    "a_gG_out_normalised = csr_row_norm(a_gG_out)\n",
    "# in matrix normalisation:\n",
    "a_gG_in_normalised = csr_row_norm(a_gG_in)\n",
    "\n",
    "\n",
    "#######\n",
    "# remove: redundant columns and rows \n",
    "out_col_sums = np.array(a_gG_out_normalised.sum(axis=0)).flatten()  \n",
    "out_col_keep = (out_col_sums != 0)\n",
    "in_col_sums = np.array(a_gG_in_normalised.sum(axis=0)).flatten()  \n",
    "in_col_keep = (in_col_sums != 0)\n",
    "\n",
    "out_row_sums = np.array(a_gG_out_normalised.sum(axis=1)).flatten()  \n",
    "in_row_sums = np.array(a_gG_in_normalised.sum(axis=1)).flatten()  \n",
    "out_row_keep = (out_row_sums != 0)\n",
    "in_row_keep = (in_row_sums != 0)\n",
    "row_keep = out_row_keep + in_row_keep # only false if both in/out are false...\n",
    "\n",
    "# columns and rows to keep:\n",
    "a_gG_out_normalised = a_gG_out_normalised[:, out_col_keep][row_keep,:]\n",
    "a_gG_in_normalised = a_gG_in_normalised[:, in_col_keep][row_keep,:]\n",
    "\n",
    "# column and row names to keep:\n",
    "all_col_names = np.concatenate([\n",
    "    np.char.add(GO_col[out_col_keep], '_out'),\n",
    "    np.char.add(GO_col[in_col_keep], '_in')\n",
    "])\n",
    "all_row_names = np.array(all_gene_ids)[row_keep]\n",
    "#######\n",
    "\n",
    "# construct a dataframe:\n",
    "all_norm_vec_df = pd.DataFrame(sp.sparse.hstack([a_gG_out_normalised, a_gG_in_normalised]).toarray(), columns=all_col_names, index=all_row_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe:\n",
    "if save_things:\n",
    "    all_norm_vec_df.to_parquet(grn_processed_dir + '/grn_GO_vector_multigos.parquet') # takes longer to save/load than recalculate..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe network vectorisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_things:\n",
    "    rn_datadir = '../../data/rn'\n",
    "    rn_processed_dir = rn_datadir + '/processed'\n",
    "    if not os.path.isdir(rn_processed_dir):\n",
    "        os.makedirs(rn_processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rn_df, meta_df = prep_recipe_data()\n",
    "\n",
    "cuisine_list = meta_df.cuisine.dropna().unique()\n",
    "\n",
    "recipe_list = rn_df.r_id.dropna().unique()\n",
    "\n",
    "ingredient_list = list(set(rn_df.ingredient.dropna().unique()) - {''})\n",
    "# rn_df = rn_df[rn_df['ingredient'].isin(ingredient_list)].reset_index(drop=True)\n",
    "rn_df = rn_df[rn_df['ingredient'].isin(ingredient_list)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Vietnamese', 'Indian', 'French', 'Jewish', 'Spanish_Portuguese',\n",
       "       'Central_SouthAmerican', 'Cajun_Creole', 'Thai', 'Scandinavian',\n",
       "       'Greek', 'American', 'African', 'MiddleEastern',\n",
       "       'EasternEuropean_Russian', 'Italian', 'Irish', 'Mexican',\n",
       "       'Chinese', 'German', 'Mediterranean', 'Japanese', 'Moroccan',\n",
       "       'Southern_SoulFood', 'English_Scottish', 'Asian', 'Southwestern'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuisine_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_RC_coo, recipe_row, cuisine_col = bipartite_cooarray( \\\n",
    "    df=meta_df.sort_values(['r_id', 'cuisine']), \\\n",
    "    row_col=['r_id', 'cuisine'], \\\n",
    "    weight=False, \\\n",
    "    row_order=list(recipe_list), \\\n",
    "    col_order=list(cuisine_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_RI_coo, recipe_row, ingredient_col = bipartite_cooarray( \\\n",
    "    df=rn_df.sort_values(['r_id', 'ingredient']), \\\n",
    "    row_col=['r_id', 'ingredient'], \\\n",
    "    weight=False, \n",
    "    row_order=list(recipe_list), \\\n",
    "    col_order=list(ingredient_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_CI_csr = (a_RC_coo.T @ a_RI_coo).tocsr() # csr array\n",
    "\n",
    "# normalisation:\n",
    "a_CI_csr_normalised = csr_row_norm(a_CI_csr)\n",
    "\n",
    "norm_is_c_vec_df = pd.DataFrame(a_CI_csr_normalised.toarray(), columns=ingredient_col, index=cuisine_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_IC_csr = a_CI_csr.T\n",
    "\n",
    "# standarisation - correct for the number of recipes in each cuisine:\n",
    "r_sums = np.array(a_RC_coo.tocsr().sum(axis=0)).flatten()  \n",
    "r_sums[r_sums == 0] = 1 # divide by 1 instead of zero\n",
    "inv_r_sums = sp.sparse.diags(1 / r_sums)\n",
    "standardised_a_IC_csr = a_IC_csr @ inv_r_sums\n",
    "\n",
    "# standardised_a_IC_csr = csr_col_norm(a_IC_csr) \n",
    "#### wrong - Column normalisation (dividing by total number of ingredients used in a cuisine) is not the same as dividing by the number of recipes in each cuisine.  \n",
    "\n",
    "# normalisation:\n",
    "a_IC_csr_s_normalised = csr_row_norm(standardised_a_IC_csr)\n",
    "\n",
    "cnorm_cs_i_bpt_df = pd.DataFrame(a_IC_csr_s_normalised.toarray(), columns= cuisine_col, index=ingredient_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardised_a_IC_df = pd.DataFrame(standardised_a_IC_csr.toarray(), columns= cuisine_col, index=ingredient_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_things:\n",
    "    norm_is_c_vec_df.to_parquet(f'{rn_processed_dir}/is_c_vectors.parquet')\n",
    "    cnorm_cs_i_bpt_df.to_parquet(f'{rn_processed_dir}/cs_i_vectors.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_bpt_df = pd.read_parquet('../data/recipe/old/cs_i_bpt_counts_bp_counts_vec.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df = pd.DataFrame(a_IC_csr.toarray(), columns= cuisine_col, index=ingredient_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sort_index(axis=1).sort_index(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_bpt_df.sort_index(axis=1).sort_index(axis=0).iloc[1:,:].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abs(old_bpt_df.sort_index(axis=1).sort_index(axis=0).iloc[1:,:].fillna(0) - df.sort_index(axis=1).sort_index(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abs(old_bpt_df.sort_index(axis=1).sort_index(axis=0).iloc[1:,:].fillna(0) - df.sort_index(axis=1).sort_index(axis=0)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abs(old_bpt_df.sort_index(axis=1).sort_index(axis=0).iloc[1:,:].fillna(0) - df.sort_index(axis=1).sort_index(axis=0)).sort_values('American')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_recipes = meta_df.query('cuisine==\"American\"').r_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rn_df[rn_df.r_id.isin(A_recipes)].query('ingredient==\"garlic\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_df.sort_values('r_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(rn_df[rn_df.r_id.isin(A_recipes)].query('ingredient==\"garlic\"').r_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc['garlic', 'American']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __ testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def edgelist_to_adjmat(edgelist):\n",
    "#     '''Turn an edgelist into an adjacency matrix\n",
    "    \n",
    "#     edgelist is a numpy array of source-sink pairs. \n",
    "\n",
    "#     indices is the order in which the columns/rows are ordered. \n",
    "#     '''\n",
    "\n",
    "#     sources, sinks = edgelist.T\n",
    "\n",
    "#     all_ids = set(sources) | set(sinks)\n",
    "\n",
    "#     n_all_ids = len(all_ids)\n",
    "\n",
    "#     adj_mat = np.zeros((n_all_ids,n_all_ids))\n",
    "\n",
    "#     i = 0\n",
    "#     ind_dict = {}\n",
    "#     ind_to_id = []\n",
    "#     for edge in edgelist:\n",
    "        \n",
    "#         source_edge = ind_dict.get(edge[0])\n",
    "#         if source_edge==None:\n",
    "#             ind_dict[edge[0]] = i\n",
    "#             source_edge = i\n",
    "#             ind_to_id.append(edge[0])\n",
    "#             i+=1\n",
    "#         sink_edge = ind_dict.get(edge[1])\n",
    "#         if sink_edge==None:\n",
    "#             ind_dict[edge[1]] = i\n",
    "#             sink_edge = i\n",
    "#             ind_to_id.append(edge[1])\n",
    "\n",
    "#             i+=1\n",
    "\n",
    "#         adj_mat[source_edge, sink_edge] = 1\n",
    "\n",
    "#     return adj_mat, ind_dict, ind_to_id\n",
    "        \n",
    "# def edgelist_to_bptadjmat(edgelist):\n",
    "#     '''Turn an edgelist into an bipartite adjacency matrix\n",
    "\n",
    "#     First column and second column are treated independently in the adjacency matrix\n",
    "#     DOESN't necessarily produce a square matrix.\n",
    "    \n",
    "#     edgelist is a numpy array of source-sink pairs. \n",
    "\n",
    "#     indices is the order in which the columns/rows are ordered. \n",
    "#     '''\n",
    "\n",
    "#     sources, sinks = edgelist.T\n",
    "\n",
    "#     all_ids = set(sources) | set(sinks)\n",
    "\n",
    "#     # n_all_ids = len(all_ids)\n",
    "\n",
    "#     adj_mat = np.zeros((len(set(sources)),len(set(sinks)))) # sources/sinks on first/second axis\n",
    "\n",
    "#     so_i = 0\n",
    "#     si_i = 0\n",
    "\n",
    "#     source_ind_dict = {}\n",
    "#     sink_ind_dict = {}\n",
    "\n",
    "#     source_ind_to_id = []\n",
    "#     sink_ind_to_id = []\n",
    "\n",
    "#     for edge in edgelist:\n",
    "        \n",
    "#         source_edge = source_ind_dict.get(edge[0])\n",
    "#         if source_edge==None:\n",
    "#             source_ind_dict[edge[0]] = so_i\n",
    "#             source_edge = so_i\n",
    "#             source_ind_to_id.append(edge[0])\n",
    "#             so_i+=1\n",
    "#         sink_edge = sink_ind_dict.get(edge[1])\n",
    "#         if sink_edge==None:\n",
    "#             sink_ind_dict[edge[1]] = si_i\n",
    "#             sink_edge = si_i\n",
    "#             sink_ind_to_id.append(edge[1])\n",
    "\n",
    "#             si_i+=1\n",
    "\n",
    "#         adj_mat[source_edge, sink_edge] = int(1)\n",
    "\n",
    "#     return adj_mat, source_ind_to_id, sink_ind_to_id\n",
    "        \n",
    "# # sample_edges = np.array([['a', 'x'], ['b', 'c']])\n",
    "# # edgelist_to_bptadjmat(edgelist=sample_edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_RC, a_RC_source_inds, a_RC_sink_inds = edgelist_to_bptadjmat(edgelist=meta_df.sort_values(['r_id', 'cuisine']).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = 'euclidean'\n",
    "# method = 'ward'\n",
    "# ind_to_id = meta_df.cuisine.unique()\n",
    "# Z = linkage(norm_is_c_vec_df.loc[ind_to_id, ingredient_list], metric=metric, method=method) ####### linkage\n",
    "\n",
    "# fig = plt.figure(figsize=(6, 15))\n",
    "# # fig.suptitle(f'{M} neighbours', fontsize=16)\n",
    "\n",
    "# gs0 = GridSpec(1,2, figure=fig,width_ratios=[35,1], wspace=0.05)\n",
    "# gs1 = GridSpecFromSubplotSpec(2,1, subplot_spec=gs0[0],\n",
    "#                                                   height_ratios=[7,1],\n",
    "#                                                   hspace=0.05)\n",
    "# ax_col_dendrogram = fig.add_subplot(gs1[0])\n",
    "# col_dendrogram = dendrogram(Z, ax=ax_col_dendrogram, color_threshold=0, above_threshold_color='black', labels=ind_to_id, orientation='right')\n",
    "# xind = col_dendrogram['leaves']\n",
    "# xmin,xmax = ax_col_dendrogram.get_xlim()\n",
    "# plt.gca()\n",
    "# plt.rcParams['axes.spines.left'] = False\n",
    "# plt.rcParams['axes.spines.right'] = False\n",
    "# plt.rcParams['axes.spines.top'] = False\n",
    "# plt.rcParams['axes.spines.bottom'] = False\n",
    "# ax_col_dendrogram.set_xticks([])\n",
    "# plt.yticks(size=25)\n",
    "\n",
    "# # plt.xticks(rotation=90, ha='center', size=30)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = 'euclidean'\n",
    "# method = 'ward'\n",
    "# ind_to_id = ingredient_list\n",
    "# Z = linkage(cnorm_cs_i_bpt_df.loc[ingredient_list].sort_index(axis=1), metric=metric, method=method) ####### linkage\n",
    "\n",
    "# ax = sns.clustermap(\n",
    "\n",
    "#     cnorm_cs_i_bpt_df.loc[ingredient_list].sort_index(axis=1), metric=metric, method=method,\n",
    "#     figsize=(10, 35),\n",
    "#     # row_cluster=False,\n",
    "#     dendrogram_ratio=(.5, .0),\n",
    "#     yticklabels=True, xticklabels=True,cmap='rocket_r', standard_scale=None,\n",
    "#     cbar_kws={'ticks':[0,0.5, 1]},\n",
    "#     cbar_pos=(0.15, 0.07, .1, .15), col_cluster=False\n",
    "# )\n",
    "\n",
    "# cbar = ax.ax_cbar\n",
    "\n",
    "# # here set the labelsize by 20\n",
    "# cbar.tick_params(labelsize=15)\n",
    "# # plt.savefig('./figs/2024/s_cs_i_clustermap.png', bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_things:\n",
    "    fw_datadir = '../../data/fw'\n",
    "    fw_processed_dir = fw_datadir + '/processed'\n",
    "    if not os.path.isdir(fw_processed_dir):\n",
    "        os.makedirs(fw_processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vectorisation(edge_df, meta_df, edge_row_cols, meta_row_cols, \\\n",
    "#                   edge_weight=True, meta_weight=True, id_order=None, \\\n",
    "#                   redundant_remove=False):\n",
    "\n",
    "#     adj_AA_coo, _id_order = adjacency_cooarray(df=edge_df, \n",
    "#                                                row_col=edge_row_cols, \n",
    "#                                                id_order=id_order, \n",
    "#                                                weight=edge_weight, \n",
    "#                                                directed=True) \n",
    "    \n",
    "#     bpt_AB_coo, row_order, col_order = bipartite_cooarray(df=meta_df, \n",
    "#                                                           row_col=meta_row_cols, \n",
    "#                                                           weight=meta_weight, \n",
    "#                                                           row_order=_id_order)\n",
    "\n",
    "#     adj_AA_out = (adj_AA_coo @ bpt_AB_coo).tocsr() # out matrix\n",
    "#     adj_AA_in = (adj_AA_coo.T @ bpt_AB_coo).tocsr() # in matrix\n",
    "\n",
    "#     # out matrix normalisation:\n",
    "#     adj_AA_out_normalised = csr_row_norm(adj_AA_out)\n",
    "\n",
    "#     # in matrix normalisation:\n",
    "#     adj_AA_in_normalised = csr_row_norm(adj_AA_in)\n",
    "\n",
    "#     # # put together into one dataframe:\n",
    "#     # all_col_names = np.concatenate([col_order + '_out', col_order + '_in'])\n",
    "#     # all_norm_vec_df = pd.DataFrame(sp.sparse.hstack([adj_AA_out_normalised, adj_AA_in_normalised]).toarray(), \\\n",
    "#     #                             columns=all_col_names, index=row_order)\n",
    "\n",
    "#     # by default, keep all:\n",
    "#     out_col_keep, in_col_keep = [[True for _ in col_order] for i in range(2)]\n",
    "#     row_keep = [True for _ in row_order]\n",
    "\n",
    "#     if redundant_remove:\n",
    "#         # remove: redundant columns and rows that are empty\n",
    "#         out_col_sums = np.array(adj_AA_out_normalised.sum(axis=0)).flatten()  \n",
    "#         out_col_keep = (out_col_sums != 0)\n",
    "#         in_col_sums = np.array(adj_AA_in_normalised.sum(axis=0)).flatten()  \n",
    "#         in_col_keep = (in_col_sums != 0)\n",
    "\n",
    "#         out_row_sums = np.array(adj_AA_out_normalised.sum(axis=1)).flatten()  \n",
    "#         in_row_sums = np.array(adj_AA_in_normalised.sum(axis=1)).flatten()  \n",
    "#         out_row_keep = (out_row_sums != 0)\n",
    "#         in_row_keep = (in_row_sums != 0)\n",
    "#         row_keep = out_row_keep + in_row_keep # only false if both in/out are false...\n",
    "\n",
    "#     # columns and rows to keep:\n",
    "#     adj_AA_out_normalised = adj_AA_out_normalised[:, out_col_keep][row_keep,:]\n",
    "#     adj_AA_in_normalised = adj_AA_in_normalised[:, in_col_keep][row_keep,:]\n",
    "\n",
    "#     # column and row names to keep:\n",
    "#     all_col_names = np.concatenate([col_order[out_col_keep] + '_out', \n",
    "#                                     col_order[in_col_keep] + '_in'])\n",
    "#     all_row_names = np.array(row_order)[row_keep]\n",
    "\n",
    "#     # construct a dataframe:\n",
    "#     all_norm_vec_df = pd.DataFrame(sp.sparse.hstack([adj_AA_out_normalised, adj_AA_in_normalised]).toarray(), \\\n",
    "#                                    columns=all_col_names, index=all_row_names)\n",
    "#     return all_norm_vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_df, meta_df_filtered, meta_fine_df_filtered, meta_fine_df_2_filtered = prep_foodweb_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "preys = set(fw_df.prey)\n",
    "predators = set(fw_df.predator)\n",
    "\n",
    "all_animals = sorted(list(preys | predators))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f for fine\n",
    "# ff for finer\n",
    "# fff for finest\n",
    "\n",
    "prep = zip((meta_df_filtered, meta_fine_df_filtered, meta_fine_df_2_filtered), ('f', 'ff', 'fff'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, data in enumerate(prep):\n",
    "    meta = data[0]\n",
    "    label = data[1]\n",
    "\n",
    "        \n",
    "    bpt_AS_coo, animal_row, type_col = bipartite_cooarray(df=meta.sort_values(['node', 'type']), row_col=['node', 'type'], weight=False, row_order=all_animals)\n",
    "    a_AA_coo, _ = adjacency_cooarray(df=fw_df, row_col=['prey', 'predator'], id_order=all_animals, weight=False, directed=True)\n",
    "\n",
    "    a_AS_out = (a_AA_coo @ bpt_AS_coo).tocsr() # out matrix\n",
    "    a_AS_in = (a_AA_coo.T @ bpt_AS_coo).tocsr() # in matrix\n",
    "\n",
    "    # out matrix normalisation:\n",
    "    a_AS_out_normalised = csr_row_norm(a_AS_out)\n",
    "\n",
    "    # in matrix normalisation:\n",
    "    a_AS_in_normalised = csr_row_norm(a_AS_in)\n",
    "\n",
    "    # put together into one dataframe:\n",
    "    all_col_names = np.concatenate([\n",
    "        np.char.add(type_col, '_out'),\n",
    "        np.char.add(type_col, '_in')\n",
    "    ])\n",
    "    all_norm_vec_df = pd.DataFrame(sp.sparse.hstack([a_AS_out_normalised, a_AS_in_normalised]).toarray(), columns=all_col_names, index=animal_row)\n",
    "    \n",
    "    if save_things:\n",
    "        all_norm_vec_df.to_parquet(f'{fw_processed_dir}/{label}_vectors.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# bpt_AS_coo, animal_row, finest_type_col = bipartite_cooarray(df=meta_fine_df_2_filtered.sort_values(['node', 'type']), row_col=['node', 'type'], weight=False, row_order=all_animals)\n",
    "# a_AA_coo, _ = adjacency_cooarray(df=fw_df, row_col=['prey', 'predator'], id_order=all_animals, weight=False, directed=True)\n",
    "\n",
    "# a_AS_out = (a_AA_coo @ bpt_AS_coo).tocsr() # out matrix\n",
    "# a_AS_in = (a_AA_coo.T @ bpt_AS_coo).tocsr() # in matrix\n",
    "\n",
    "# # out matrix normalisation:\n",
    "# a_AS_out_normalised = csr_row_norm(a_AS_out)\n",
    "\n",
    "# # in matrix normalisation:\n",
    "# a_AS_in_normalised = csr_row_norm(a_AS_in)\n",
    "\n",
    "# # put together into one dataframe:\n",
    "# all_col_names = np.concatenate([finest_type_col + '_out', finest_type_col + '_in'])\n",
    "# all_norm_vec_df = pd.DataFrame(sp.sparse.hstack([a_AS_out_normalised, a_AS_in_normalised]).toarray(), columns=all_col_names, index=animal_row)\n",
    "\n",
    "# all_norm_vec_df.to_parquet('../data/foodweb/processed/finest_vectors.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Drosophila_ VNC connectome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.data_prep_drosophila import drosophila_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadate = '20241119' # date of drosophila VNC data pull\n",
    "dm_datadir = f'../../data/{datadate}_dm_data'\n",
    "\n",
    "if save_things:\n",
    "    dm_processed_dir = dm_datadir + '/processed'\n",
    "    if not os.path.isdir(dm_processed_dir):\n",
    "        os.makedirs(dm_processed_dir)\n",
    "\n",
    "\n",
    "df, meta_df = drosophila_data(datadate=datadate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_cat_string = 'hemilineage'\n",
    "# hemilineage_fileprefix = f'{dm_datadir}/processed/hemilineage'\n",
    "\n",
    "h_cat_meta = meta_df[['bodyId', h_cat_string]].rename(columns={'bodyId':'id', h_cat_string:'type'}).copy(True)\n",
    "n_ids = h_cat_meta.id.sort_values().unique() # all neuron ids\n",
    "\n",
    "h_cat_meta.dropna(inplace=True)# remove nans\n",
    "remove = ['TBD'] # remove TBD type as well\n",
    "h_cat_meta = h_cat_meta[~h_cat_meta['type'].isin(remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_NN_coo, _ = adjacency_cooarray(df=df, row_col=['bodyId_pre', 'bodyId_post'], id_order=n_ids, weight=True, directed=True)\n",
    "# 24s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpt_NH_coo, neuron_row, type_col = bipartite_cooarray(df=h_cat_meta, row_col=['id', 'type'], weight=False, row_order=n_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_NH_out = (a_NN_coo @ bpt_NH_coo).tocsr() # out matrix\n",
    "a_NH_in = (a_NN_coo.T @ bpt_NH_coo).tocsr() # in matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out matrix normalisation:\n",
    "a_NH_out_normalised = csr_row_norm(a_NH_out)\n",
    "\n",
    "# in matrix normalisation:\n",
    "a_NH_in_normalised = csr_row_norm(a_NH_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_col_names = np.concatenate([\n",
    "    np.char.add(type_col, '_out'),\n",
    "    np.char.add(type_col, '_in')\n",
    "])\n",
    "all_norm_vec_df = pd.DataFrame(sp.sparse.hstack([a_NH_out_normalised, a_NH_in_normalised]).toarray(), columns=all_col_names, index=neuron_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if save_things:\n",
    "    all_norm_vec_df.to_parquet(f'{dm_processed_dir}/{h_cat_string}_vec.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _C. elegans_ connectome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.data_prep_celegans import celegans_data, syn_to_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssm47/Desktop/thesis_code/01_vectorisation/src/module/data_prep_celegans.py:74: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  edge_df = pd.read_csv('../../data/celegans/white_1986_whole.csv', delim_whitespace=True)\n"
     ]
    }
   ],
   "source": [
    "edge_df, ce_meta = celegans_data()\n",
    "\n",
    "chem_edges_df = syn_to_edge(edge_df, electrical=False)\n",
    "all_edges_df = syn_to_edge(edge_df, electrical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_things:\n",
    "    ce_datadir = f'../../data/celegans'\n",
    "    ce_processed_dir = ce_datadir + '/processed'\n",
    "    if not os.path.isdir(grn_processed_dir):\n",
    "        os.makedirs(grn_processed_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings_list = ['Anatomical cell class (WW Barry)', 'Cook cell category', 'Cell Class', 'Final classification']\n",
    "\n",
    "shorthand = {'Anatomical cell class (WW Barry)':'cellclass', \\\n",
    "             'Cook cell category':'cook_ccat', \\\n",
    "             'Cell Class':'ncclass', 'Final classification':'fclass'}\n",
    "\n",
    "edge_dict = {'chem':chem_edges_df, 'all':all_edges_df}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_string in strings_list:\n",
    "    class_meta = ce_meta.reset_index()[['Neuron', class_string]].rename(columns={'Neuron':'id', class_string:'type'}).copy(True)\n",
    "    class_name = shorthand.get(class_string)\n",
    "    if save_things:\n",
    "        todir = f'../../data/celegans/processed/{class_name}'\n",
    "        if not os.path.isdir(todir):\n",
    "            os.mkdir(todir)\n",
    "    n_ids = class_meta.id.sort_values().unique()\n",
    "\n",
    "    for edgetype in edge_dict.keys():\n",
    "        typeedge_df = edge_dict.get(edgetype)\n",
    "        a_NN_coo, _ = adjacency_cooarray(\\\n",
    "            df=typeedge_df,\\\n",
    "            row_col=['pre', 'post'],\\\n",
    "            id_order=n_ids,\\\n",
    "            weight=True,\\\n",
    "            directed=True\\\n",
    "            )\n",
    "\n",
    "        bpt_NT_coo, neuron_row, type_col = bipartite_cooarray(\n",
    "            df=class_meta,\\\n",
    "            row_col=['id', 'type'],\\\n",
    "            weight=False,\\\n",
    "            row_order=n_ids\\\n",
    "            )\n",
    "        \n",
    "        a_NT_out = (a_NN_coo @ bpt_NT_coo).tocsr() # out matrix\n",
    "        a_NT_in = (a_NN_coo.T @ bpt_NT_coo).tocsr() # in matrix\n",
    "        # out matrix normalisation:\n",
    "        a_NT_out_normalised = csr_row_norm(a_NT_out)\n",
    "        # in matrix normalisation:\n",
    "        a_NT_in_normalised = csr_row_norm(a_NT_in)\n",
    "\n",
    "        all_col_names = np.concatenate([\n",
    "            np.char.add(type_col, '_out'),\n",
    "            np.char.add(type_col, '_in')\n",
    "        ])\n",
    "\n",
    "        all_norm_vec_df = pd.DataFrame(sp.sparse.hstack([a_NT_out_normalised, a_NT_in_normalised]).toarray(), columns=all_col_names, index=neuron_row)\n",
    "        if save_things:\n",
    "            all_norm_vec_df.to_parquet(f'{todir}/V_{class_name}_{edgetype}_vec.parquet')\n",
    "        # todir = f'../../data/celegans/processed/{class_name}'\n",
    "        # save_dir = f'{todir}/V_{class_name}_{edgetype}_vec.parquet'\n",
    "        # test_df = pd.read_parquet(save_dir)\n",
    "        # print((test_df - all_norm_vec_df).fillna(0).sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_string = 'Cook cell category'\n",
    "# typeedge_df = all_edges_df\n",
    "# class_meta = ce_meta.reset_index()[['Neuron', class_string]].rename(columns={'Neuron':'id', class_string:'type'}).copy(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ids = class_meta.id.sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_NN_coo, _ = adjacency_cooarray(df=typeedge_df, row_col=['pre', 'post'], id_order=n_ids, weight=True, directed=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpt_NT_coo, neuron_row, type_col = bipartite_cooarray(df=class_meta, row_col=['id', 'type'], weight=False, row_order=n_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_NT_out = (a_NN_coo @ bpt_NT_coo).tocsr() # out matrix\n",
    "a_NT_in = (a_NN_coo.T @ bpt_NT_coo).tocsr() # in matrix\n",
    "\n",
    "# out matrix normalisation:\n",
    "a_NT_out_normalised = csr_row_norm(a_NT_out)\n",
    "\n",
    "# in matrix normalisation:\n",
    "a_NT_in_normalised = csr_row_norm(a_NT_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_col_names = np.concatenate([\n",
    "    np.char.add(type_col, '_out'),\n",
    "    np.char.add(type_col, '_in')\n",
    "])\n",
    "all_norm_vec_df = pd.DataFrame(sp.sparse.hstack([a_NT_out_normalised, a_NT_in_normalised]).toarray(), columns=all_col_names, index=neuron_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if save_things:\n",
    "#     all_norm_vec_df.to_parquet(f'{c}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
